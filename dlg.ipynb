{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpfrdxElvQUeVY57XapyZY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bahador1/I-GOS/blob/main/dlg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jB8W3HZsvCf"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import pickle\n",
        "import PIL.Image as Image\n",
        "\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, channel=3, hideen=768, num_classes=10):\n",
        "        super(LeNet, self).__init__()\n",
        "        act = nn.Sigmoid\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Conv2d(channel, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
        "            act(),\n",
        "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
        "            act(),\n",
        "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=1),\n",
        "            act(),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hideen, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.body(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_from_Image(Dataset):\n",
        "    def __init__(self, imgs, labs, transform=None):\n",
        "        self.imgs = imgs # img paths\n",
        "        self.labs = labs # labs is ndarray\n",
        "        self.transform = transform\n",
        "        del imgs, labs\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labs.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        lab = self.labs[idx]\n",
        "        img = Image.open(self.imgs[idx])\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "        img = self.transform(img)\n",
        "        return img, lab\n"
      ],
      "metadata": {
        "id": "DFA1uyInsxhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'lfw'\n",
        "root_path = '.'\n",
        "data_path = os.path.join(root_path, './data').replace('\\\\', '/')\n",
        "save_path = os.path.join(root_path, 'results/DLG_%s'%dataset).replace('\\\\', '/')\n",
        "\n",
        "lr = 1.0\n",
        "num_dummy = 1\n",
        "Iteration = 300\n",
        "num_exp = 1000\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = 'cuda' if use_cuda else 'cpu'\n",
        "\n",
        "tt = transforms.Compose([transforms.ToTensor()])\n",
        "tp = transforms.Compose([transforms.ToPILImage()])\n",
        "\n",
        "print(dataset, 'root_path:', root_path)\n",
        "print(dataset, 'data_path:', data_path)\n",
        "print(dataset, 'save_path:', save_path)\n",
        "\n",
        "if not os.path.exists('results'):\n",
        "    os.mkdir('results')\n",
        "if not os.path.exists(save_path):\n",
        "    os.mkdir(save_path)\n",
        "\n",
        "\n",
        "\n",
        "''' load data '''\n",
        "if dataset == 'MNIST':\n",
        "    shape_img = (28, 28)\n",
        "    num_classes = 10\n",
        "    channel = 1\n",
        "    hidden = 588\n",
        "    dst = datasets.MNIST(data_path, download=False)\n",
        "\n",
        "elif dataset == 'cifar100':\n",
        "    shape_img = (32, 32)\n",
        "    num_classes = 100\n",
        "    channel = 3\n",
        "    hidden = 768\n",
        "    dst = datasets.CIFAR100(data_path, download=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "''' train DLG '''\n",
        "for idx_net in range(num_exp):\n",
        "    net = LeNet(channel=channel, hideen=hidden, num_classes=num_classes)\n",
        "    # net.apply(weights_init)\n",
        "\n",
        "    print('running %d|%d experiment'%(idx_net, num_exp))\n",
        "    net = net.to(device)\n",
        "    idx_shuffle = np.random.permutation(len(dst))\n",
        "\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    imidx_list = []\n",
        "\n",
        "    for imidx in range(num_dummy):\n",
        "        idx = idx_shuffle[imidx]\n",
        "        imidx_list.append(idx)\n",
        "        tmp_datum = tt(dst[idx][0]).float().to(device)\n",
        "        tmp_datum = tmp_datum.view(1, *tmp_datum.size())\n",
        "        tmp_label = torch.Tensor([dst[idx][1]]).long().to(device)\n",
        "        tmp_label = tmp_label.view(1, )\n",
        "        if imidx == 0:\n",
        "            gt_data = tmp_datum\n",
        "            gt_label = tmp_label\n",
        "        else:\n",
        "            gt_data = torch.cat((gt_data, tmp_datum), dim=0)\n",
        "            gt_label = torch.cat((gt_label, tmp_label), dim=0)\n",
        "\n",
        "\n",
        "    # compute original gradient\n",
        "    out = net(gt_data)\n",
        "    y = criterion(out, gt_label)\n",
        "    dy_dx = torch.autograd.grad(y, net.parameters())\n",
        "    original_dy_dx = list((_.detach().clone() for _ in dy_dx))\n",
        "\n",
        "    # generate dummy data and label\n",
        "    dummy_data = torch.randn(gt_data.size()).to(device).requires_grad_(True)\n",
        "    dummy_label = torch.randn((gt_data.shape[0], num_classes)).to(device).requires_grad_(True)\n",
        "\n",
        "    optimizer = torch.optim.LBFGS([dummy_data, dummy_label], lr=lr)\n",
        "\n",
        "\n",
        "    history = []\n",
        "    history_iters = []\n",
        "    losses = []\n",
        "    mses = []\n",
        "    train_iters = []\n",
        "\n",
        "    print('lr =', lr)\n",
        "    for iters in range(Iteration):\n",
        "\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            pred = net(dummy_data)\n",
        "            dummy_loss = - torch.mean(torch.sum(torch.softmax(dummy_label, -1) * torch.log(torch.softmax(pred, -1)), dim=-1))\n",
        "            # dummy_loss = criterion(pred, gt_label)\n",
        "\n",
        "\n",
        "            dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
        "\n",
        "            grad_diff = 0\n",
        "            for gx, gy in zip(dummy_dy_dx, original_dy_dx):\n",
        "                grad_diff += ((gx - gy) ** 2).sum()\n",
        "            grad_diff.backward()\n",
        "            return grad_diff\n",
        "\n",
        "        optimizer.step(closure)\n",
        "        current_loss = closure().item()\n",
        "        train_iters.append(iters)\n",
        "        losses.append(current_loss)\n",
        "        mses.append(torch.mean((dummy_data-gt_data)**2).item())\n",
        "\n",
        "\n",
        "        if iters % int(Iteration / 30) == 0:\n",
        "            current_time = str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n",
        "            print(current_time, iters, 'loss = %.8f, mse = %.8f' %(current_loss, mses[-1]))\n",
        "            history.append([tp(dummy_data[imidx].cpu()) for imidx in range(num_dummy)])\n",
        "            history_iters.append(iters)\n",
        "\n",
        "            for imidx in range(num_dummy):\n",
        "                plt.figure(figsize=(12, 8))\n",
        "                plt.subplot(3, 10, 1)\n",
        "                plt.imshow(tp(gt_data[imidx].cpu()))\n",
        "                for i in range(min(len(history), 29)):\n",
        "                    plt.subplot(3, 10, i + 2)\n",
        "                    plt.imshow(history[i][imidx])\n",
        "                    plt.title('iter=%d' % (history_iters[i]))\n",
        "                    plt.axis('off')\n",
        "\n",
        "                plt.savefig('%s/DLG_on_%s_%05d.png' % (save_path, imidx_list, imidx_list[imidx]))\n",
        "                plt.close()\n",
        "\n",
        "\n",
        "            if current_loss < 0.000001: # converge\n",
        "                break\n",
        "\n",
        "        loss_DLG = losses\n",
        "        label_DLG = torch.argmax(dummy_label, dim=-1).detach().item()\n",
        "        mse_DLG = mses\n",
        "\n",
        "\n",
        "    print('imidx_list:', imidx_list)\n",
        "    print('loss_DLG:', loss_DLG[-1],)\n",
        "    print('mse_DLG:', mse_DLG[-1],)\n",
        "    print('gt_label:', gt_label.detach().cpu().data.numpy(), 'lab_DLG:', label_DLG)\n",
        "\n",
        "    print('----------------------\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "fEWxYEjZszz3",
        "outputId": "4099c88c-d35f-458f-8e50-570786077b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lfw root_path: .\n",
            "lfw data_path: ././data\n",
            "lfw save_path: ./results/DLG_lfw\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'channel' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2604711784.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m''' train DLG '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx_net\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhideen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'channel' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1-M33maw1yjI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}